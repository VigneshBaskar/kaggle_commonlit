{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e016d86-b9fb-4bb5-b856-dfda28215b45",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this article [Shahul](https://shahules786.github.io/) and I ([Vignesh Baskaran](https://vigneshbaskar.github.io/)) try to summarize our learnings from the Kaggle Commonlit competition. We are happy that we managed to secure the 40th position in this competition amongst 3633 teams. But deep in our heart we firmly believe it doesn't matter which position one ends up in, but what matters is the learnings each one of us get out of it. We respect every single participant in this competition. We wouldn't have managed to learn so much without all of you. Thank you very much.\n",
    "\n",
    "This competition was organized by Commonlit. CommonLit is a nonprofit education technology organization dedicated to ensuring that all students, especially students in Title I schools, graduate with the reading, writing, communication, and problem-solving skills they need to be successful in college and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03281a85-3ffe-4feb-96b3-8c90511e3c5e",
   "metadata": {},
   "source": [
    "# Competition objective\n",
    "In this competition we are provided with a list of short text excerpts and their corresponding score which indicates how difficult it is to read the text. These scores were computed using Bradley Terry method in which pairs of texts are compared by several examiners and they manually classify which text among the pair is difficult to read. For instance here is a pair of text:\n",
    "\n",
    "**Text 1**:\n",
    "> Every day, Emeka's father took him to school in his car. He also brought Emeka home after school. One afternoon on their way home, Emeka's father stopped to buy something at a big shop. From the car, Emeka looked across the road and saw an old man. He was carrying a big load on his head. He was tired and walked slowly. Emeka kept looking at him. The old man sat under the shade of a tree on the walkway and opened his bag.\n",
    "\n",
    "**Text 2**:\n",
    ">The nature of incrustation and the evils resulting therefrom having been stated, it now remains to consider the methods that have been devised to overcome them. These methods naturally resolve themselves into two kinds, chemical and mechanical. The chemical method has two modifications; in one the design is to purify the water in large tanks or reservoirs, by the addition of certain substances which shall precipitate all the scale-forming ingredients before the water is fed into the boiler; in the other the chemical agent is fed into the boiler from time to time, and the object is to effect the precipitation of the saline matter in such a manner that it will not form solid masses of adherent scale\n",
    "\n",
    "Here are the ratings offered by 5 examiners who compared this pair of text  \n",
    "\n",
    "|            | Easy Text |\n",
    "|------------|-----------|\n",
    "| Examiner 1 | Text 1    |\n",
    "| Examiner 2 | Text 1    |\n",
    "| Examiner 3 | Text 1    |\n",
    "| Examiner 4 | Text 2    |\n",
    "| Examiner 5 | Text 1    |\n",
    "\n",
    "In this way several pairs of texts are compared by multiple examiners and based on their labels, scores are assigned to each text excerpt. The scores range from (-3, 2). The higher the score is the easier it is to read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db269881-2d23-4bb5-893b-bb243f8b9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "from transformers import AdamW, AutoTokenizer\n",
    "from commonlit_nn_kit import clear_cuda, get_scheduler, get_optimizer_parameters, create_uno_text_dataloader, RobertaMaskAddedAttentionHeadRegressor\n",
    "from commonlit_nn_kit import compute_mse_loss, compute_rmse_loss, compute_rmse_score, forward_pass_uno_text_batch, Saver, UnoStacker, train_and_evaluate\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604a006a-1a42-481f-8bc0-828e2bb1bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'seed': 42}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95e350-ca39-4f59-bfce-caa24376da1d",
   "metadata": {},
   "source": [
    "# Competition data\n",
    "Here is a sample of competition data. We split the data into five folds in a stratified fashion based on the target scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4a5381-7f95-4f8b-b918-5d4aff251ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Alice looked at the jury-box, and saw that, in her haste, she had put the Lizard in head downwards, and the poor little thing was waving its tail about in a melancholy way, being quite unable to move. She soon got it out again, and put it right; ‘not that it signifies much,' she said to herself; ‘I should think it would be quite as much use in the trial one way up as the other.'\\nAs soon as the jury had a little recovered from the shock of being upset, and their slates and pencils had been found and handed back to them, they set to work very diligently to write out a history of the accident, all except the Lizard, who seemed too much overcome to do anything but sit with its mouth open, gazing up into the roof of the court.\\n‘What do you know about this business?' the King said to Alice.\\n‘Nothing,' said Alice.\\n‘Nothing whatever?' persisted the King.\\n‘Nothing whatever,' said Alice.</td>\n",
       "      <td>-0.432678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>Artificial intelligence (AI) is intelligence exhibited by machines. In computer science, an ideal \"intelligent\" machine is a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\". As machines become increasingly capable, facilities once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an exemplar of \"artificial intelligence\" having become a routine technology. Capabilities still classified as AI include advanced Chess and Go systems and self-driving cars.\\nAI research is divided into subfields that focus on specific problems or on specific approaches or on the use of a particular tool or towards satisfying particular applications.</td>\n",
       "      <td>-1.161746</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      excerpt  \\\n",
       "2089                                                                                          Alice looked at the jury-box, and saw that, in her haste, she had put the Lizard in head downwards, and the poor little thing was waving its tail about in a melancholy way, being quite unable to move. She soon got it out again, and put it right; ‘not that it signifies much,' she said to herself; ‘I should think it would be quite as much use in the trial one way up as the other.'\\nAs soon as the jury had a little recovered from the shock of being upset, and their slates and pencils had been found and handed back to them, they set to work very diligently to write out a history of the accident, all except the Lizard, who seemed too much overcome to do anything but sit with its mouth open, gazing up into the roof of the court.\\n‘What do you know about this business?' the King said to Alice.\\n‘Nothing,' said Alice.\\n‘Nothing whatever?' persisted the King.\\n‘Nothing whatever,' said Alice.   \n",
       "2806  Artificial intelligence (AI) is intelligence exhibited by machines. In computer science, an ideal \"intelligent\" machine is a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\". As machines become increasingly capable, facilities once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an exemplar of \"artificial intelligence\" having become a routine technology. Capabilities still classified as AI include advanced Chess and Go systems and self-driving cars.\\nAI research is divided into subfields that focus on specific problems or on specific approaches or on the use of a particular tool or towards satisfying particular applications.   \n",
       "\n",
       "        target  fold  \n",
       "2089 -0.432678     2  \n",
       "2806 -1.161746     3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "competition_data = pd.read_csv(\"commonlit-splits/commonlittrain_stratified_simple.csv\")[['excerpt', 'target', 'fold']]\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(competition_data.sample(2, random_state=config['seed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e237f80f-144d-4caa-affe-596962dc4546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(competition_data.target.to_numpy(),  edgecolor='black', linewidth=1.2)\n",
    "plt.title('Target scores histogram')\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359b9f4-0953-4295-a398-f1351cec9ff1",
   "metadata": {},
   "source": [
    "### Very easy text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a30f85-1d08-45dc-a69c-a6ab46402c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>For her last birthday, Sisanda had a special treat – her parents got permission for her to have a party at the game reserve. The giraffes at the reserve were curious about this group of people. They stretched out their long necks for the best view of the party and they even seemed to want some of the birthday cake! Sisanda loved the giraffes. All animals were special to her, but it was the quiet and gentle giraffes that stole her heart. She could spend all day watching them. \\nOne Friday, Sisanda's father came home from work early. He looked very upset. \"What's wrong, Baba?\" Sisanda asked. \"Today a swarm of bees stung a mother giraffe,\" explained Sisanda's father. \"Her head was so swollen from all the stings that her beautiful eyes were closed. We tried everything to help her, but it was no use – she died. And the saddest part of all is that she had a young calf that still needs her.\"</td>\n",
       "      <td>1.59787</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               excerpt  \\\n",
       "981  For her last birthday, Sisanda had a special treat – her parents got permission for her to have a party at the game reserve. The giraffes at the reserve were curious about this group of people. They stretched out their long necks for the best view of the party and they even seemed to want some of the birthday cake! Sisanda loved the giraffes. All animals were special to her, but it was the quiet and gentle giraffes that stole her heart. She could spend all day watching them. \\nOne Friday, Sisanda's father came home from work early. He looked very upset. \"What's wrong, Baba?\" Sisanda asked. \"Today a swarm of bees stung a mother giraffe,\" explained Sisanda's father. \"Her head was so swollen from all the stings that her beautiful eyes were closed. We tried everything to help her, but it was no use – she died. And the saddest part of all is that she had a young calf that still needs her.\"   \n",
       "\n",
       "      target  fold  \n",
       "981  1.59787     3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(competition_data.query(\"target>1.5\").sample(1, random_state=config['seed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd02e8-a560-49ab-b295-27bb53ecf87e",
   "metadata": {},
   "source": [
    "### Very difficult text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c90ea4-7466-4b86-a463-6713d8864a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>The light-year is a unit of length used to express astronomical distances and measures about 9.46 trillion kilometres (9.46 x 1012 km) or 5.88 trillion miles (5.88 x 1012 mi). As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in vacuum in one Julian year (365.25 days).Because it includes the word \"year\", the term light-year may be misinterpreted as a unit of time.\\nThe light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist and popular science publications. The unit most commonly used in professional astrometry is the parsec (symbol: pc, about 3.26 light-years; the distance at which one astronomical unit subtends an angle of one second of arc).\\n As defined by the IAU, the light-year is the product of the Julian year (365.25 days as opposed to the 365.2425-day Gregorian year) and the speed of light (299792458 m/s). Both of these values are included in the IAU (1976) System of Astronomical Constants, used since 1984.</td>\n",
       "      <td>-3.256312</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      excerpt  \\\n",
       "457  The light-year is a unit of length used to express astronomical distances and measures about 9.46 trillion kilometres (9.46 x 1012 km) or 5.88 trillion miles (5.88 x 1012 mi). As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in vacuum in one Julian year (365.25 days).Because it includes the word \"year\", the term light-year may be misinterpreted as a unit of time.\\nThe light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist and popular science publications. The unit most commonly used in professional astrometry is the parsec (symbol: pc, about 3.26 light-years; the distance at which one astronomical unit subtends an angle of one second of arc).\\n As defined by the IAU, the light-year is the product of the Julian year (365.25 days as opposed to the 365.2425-day Gregorian year) and the speed of light (299792458 m/s). Both of these values are included in the IAU (1976) System of Astronomical Constants, used since 1984.   \n",
       "\n",
       "       target  fold  \n",
       "457 -3.256312     2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(competition_data.query(\"target<-3\").sample(1, random_state=config['seed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba009050-7b51-4db3-8225-a28a35038de0",
   "metadata": {},
   "source": [
    "## Stratified Kfold splitting vs Simple Kfold splitting\n",
    "The Simple Kfold splitting randomly splits the data into folds, whereas the stratified Kfold splitting takes group information into account to avoid building folds with imbalanced class distributions. In our case we experimented with training models on both simple splitting and stratified splitting. We observed that for the Roberta large model the public LB increased with stratified Kfold whereas with base model the public LB increased with simple Kfold. Although our gut feeling says that stratified Kfold is the right way to go about we had a couple of models in our ensemble trained using simple Kfold just to make it diversified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b4e31-97dd-495a-8de0-e697788fb976",
   "metadata": {},
   "source": [
    "## Training data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed56c02-b5b7-4c04-98bc-d7ade3cedae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training data: 2834\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of samples in the training data: {len(competition_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8523e-5fa0-4d58-96c4-9257f956225d",
   "metadata": {},
   "source": [
    "Owing to the very small size of the dataset we were worried that our models would overfit the data. Therefore we looked into several ways to prevent overfitting. One of the solutions which worked exceptionally very well in the very last minute was pretraining the models with large amount of external data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35fbbe-0ea6-49d3-b9f6-6bc56de639bf",
   "metadata": {},
   "source": [
    "# External data\n",
    "We came to know that Wikipedia has a project called Simple Wikipedia in its umbrella. The Simple English Wikipedia is an English-language edition of the Wikipedia written primarily in Basic English and Learning English. Simple English Wikipedia's basic presentation style makes it very helpful for beginners learning English. Its simpler word structure and syntax, while detracting from the raw information standpoint, can make the information easy to understand.\n",
    "\n",
    "[Mark Wijkhuizen](https://www.kaggle.com/markwijkhuizen) published a clean version of Wikipedia and Simple Wikipedia pairs [here](https://www.kaggle.com/markwijkhuizen/simplenormal-wikipedia-sections). The column easy text comes from Simple Wikipedia and the column difficult text comes from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8898bd0e-9ea6-49f2-b5f5-27fb3ef9d587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>easy_text</th>\n",
       "      <th>difficult_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16115</th>\n",
       "      <td>Epstein–Barr virus</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>It is one of the most common viruses in humans. EBV is best known as the cause of infectious mononucleosis (glandular fever). It is also associated with some forms of cancer, such as Hodgkin's lymphoma, and conditions associated with human immunodeficiency virus (HIV). EBV may be associated with...</td>\n",
       "      <td>EBV is a double-stranded DNA virus. It is best known as the cause of infectious mononucleosis (\"mono\" or \"glandular fever\"). It is also associated with various non-malignant, premalignant, and malignant Epstein–Barr virus-associated lymphoproliferative diseases such as Burkitt lymphoma, hemophag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15807</th>\n",
       "      <td>Substitute (association football)</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Substitutions can be made for many reasons. Sometimes, they are made to replace a player who has become tired or injured. Substitutes can also replace a player who is playing poorly, or can be for tactics. For example, substitute strikers can be brought on for defenders if the team needs to atta...</td>\n",
       "      <td>Substitutions are generally made to replace a player who has become tired or injured, or who is performing poorly, or for tactical reasons (such as bringing a striker on in place of a defender). Unlike some sports (such as American football, ice hockey or kabaddi), but like in baseball, a player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22575</th>\n",
       "      <td>Ford Pinto</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>The vehicle was marketed under Ford Motor Company in the States and Canada. It was made from the 1971 to 1980 model years. Pinto was the smallest American Ford car since 1907. It was Ford's first subcompact vehicle in North America. The Pinto was manufactured in three body styles: two-door sedan...</td>\n",
       "      <td>The smallest American Ford vehicle since 1907, the Pinto was the first subcompact vehicle produced by Ford in North America. The Pinto was marketed in three body styles through its production: a two-door fastback sedan with a trunk, a three-door hatchback, and a two-door station wagon. Mercury o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title   section  \\\n",
       "16115                 Epstein–Barr virus  Abstract   \n",
       "15807  Substitute (association football)  Abstract   \n",
       "22575                         Ford Pinto  Abstract   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                         easy_text  \\\n",
       "16115  It is one of the most common viruses in humans. EBV is best known as the cause of infectious mononucleosis (glandular fever). It is also associated with some forms of cancer, such as Hodgkin's lymphoma, and conditions associated with human immunodeficiency virus (HIV). EBV may be associated with...   \n",
       "15807  Substitutions can be made for many reasons. Sometimes, they are made to replace a player who has become tired or injured. Substitutes can also replace a player who is playing poorly, or can be for tactics. For example, substitute strikers can be brought on for defenders if the team needs to atta...   \n",
       "22575  The vehicle was marketed under Ford Motor Company in the States and Canada. It was made from the 1971 to 1980 model years. Pinto was the smallest American Ford car since 1907. It was Ford's first subcompact vehicle in North America. The Pinto was manufactured in three body styles: two-door sedan...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    difficult_text  \n",
       "16115  EBV is a double-stranded DNA virus. It is best known as the cause of infectious mononucleosis (\"mono\" or \"glandular fever\"). It is also associated with various non-malignant, premalignant, and malignant Epstein–Barr virus-associated lymphoproliferative diseases such as Burkitt lymphoma, hemophag...  \n",
       "15807  Substitutions are generally made to replace a player who has become tired or injured, or who is performing poorly, or for tactical reasons (such as bringing a striker on in place of a defender). Unlike some sports (such as American football, ice hockey or kabaddi), but like in baseball, a player...  \n",
       "22575  The smallest American Ford vehicle since 1907, the Pinto was the first subcompact vehicle produced by Ford in North America. The Pinto was marketed in three body styles through its production: a two-door fastback sedan with a trunk, a three-door hatchback, and a two-door station wagon. Mercury o...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "external_data = pd.read_csv(\"data/external_data/wikipedia_sections_without_first_sentence.csv\")\n",
    "external_data = external_data[['title', 'section', 'easy_text', 'difficult_text']]\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", 300):\n",
    "    display(external_data.sample(3, random_state=config['seed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97a220-992a-4976-9e57-8988e73ed4bc",
   "metadata": {},
   "source": [
    "# Competition DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ff780e-9c70-437e-bcd0-86869944871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train_data: 2267\n",
      "Number of sample in the valid_data: 567\n"
     ]
    }
   ],
   "source": [
    "config.update({'fold': 0, 'batch_size': 4, 'apply_preprocessing': False})\n",
    "\n",
    "train_data, valid_data = competition_data[competition_data['fold'] != config['fold']], competition_data[competition_data['fold']==config['fold']]\n",
    "print(f'Number of samples in the train_data: {len(train_data)}')\n",
    "print(f'Number of sample in the valid_data: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b89f15-dc7a-481e-888b-a37c1c21d19c",
   "metadata": {},
   "source": [
    "### Dataset Definition\n",
    "```python\n",
    "class UnoTextDataset(Dataset):\n",
    "    def __init__(self, text_excerpts, targets):\n",
    "        self.text_excerpts = text_excerpts\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_excerpts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'text_excerpt': self.text_excerpts[idx],\n",
    "                  'target': self.targets[idx]}\n",
    "        return sample\n",
    "```\n",
    "\n",
    "```python\n",
    "class DuoTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    If the first input is easier to read then the target is 1\n",
    "    If the second input is easier to read then the target is -1\n",
    "    \"\"\"\n",
    "    def __init__(self, text_excerpts_left, text_excerpts_right, targets):\n",
    "        self.text_excerpts_left = text_excerpts_left\n",
    "        self.text_excerpts_right = text_excerpts_right\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'text_excerpt_left': self.text_excerpts_left[idx],\n",
    "                  'text_excerpt_right': self.text_excerpts_right[idx],\n",
    "                  'target': self.targets[idx]}\n",
    "        return sample\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc7d23b-192d-4b59-865b-0ee607dfce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the train_dataloader: 567\n",
      "Number of batches in the valid_dataloader: 142\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = create_uno_text_dataloader(data=train_data, batch_size=config['batch_size'], shuffle=True, sampler=None, apply_preprocessing=config['apply_preprocessing'])\n",
    "valid_dataloader = create_uno_text_dataloader(data=valid_data, batch_size=config['batch_size'], shuffle=False, sampler=None, apply_preprocessing=config['apply_preprocessing'])\n",
    "print(f'Number of batches in the train_dataloader: {len(train_dataloader)}')\n",
    "print(f'Number of batches in the valid_dataloader: {len(valid_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be301f02-23e7-430b-87e1-00b26c58ed3f",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "The tokenizer gives out two fields: \n",
    "1. `input_ids` of shape `(batch_size, max_num_tokens)` which maps each token to the vocabulary \n",
    "2. `attention_mask` of shape `(batch_size, max_num_tokens)` which provides info on the `<PAD>` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e34fd3a-9cb9-4e81-9f43-8b5792c87f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 'base'\n",
    "config = {'tokenizer_name': f'data/robertas/roberta-{model_size}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "627232ea-97c9-4a5c-9d9a-d0aba63d6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "Input IDs shape: torch.Size([4, 233]) \n",
      "Attention mask shape: torch.Size([4, 233]) \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=config['tokenizer_name'])\n",
    "tokenized_data = tokenizer(text=next(iter(train_dataloader))['text_excerpt'], truncation=True, padding=True, return_tensors='pt')\n",
    "print(tokenized_data.keys())\n",
    "print(f\"Input IDs shape: {tokenized_data['input_ids'].shape} \")\n",
    "print(f\"Attention mask shape: {tokenized_data['attention_mask'].shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260e336-be2c-4245-8fc3-9addc1d98785",
   "metadata": {},
   "source": [
    "# Model\n",
    "---\n",
    "```python\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, head_hidden_dim):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n",
    "        self.W = nn.Linear(input_dim, head_hidden_dim)\n",
    "        self.V = nn.Linear(head_hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_scores = self.V(torch.tanh(self.W(x)))\n",
    "        attention_scores = torch.softmax(attention_scores, dim=1)\n",
    "        attentive_x = attention_scores * x\n",
    "        attentive_x = attentive_x.sum(axis=1)\n",
    "        return attentive_x\n",
    "```\n",
    "---\n",
    "\n",
    "```python\n",
    "class MaskAddedAttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, head_hidden_dim):\n",
    "        super(MaskAddedAttentionHead, self).__init__()\n",
    "        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n",
    "        self.W = nn.Linear(input_dim, head_hidden_dim)\n",
    "        self.V = nn.Linear(head_hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, attention_mask):\n",
    "        attention_scores = self.V(torch.tanh(self.W(x)))\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        attention_scores = torch.softmax(attention_scores, dim=1)\n",
    "        attentive_x = attention_scores * x\n",
    "        attentive_x = attentive_x.sum(axis=1)\n",
    "        return attentive_x\n",
    "```\n",
    "---\n",
    "\n",
    "```python\n",
    "class RobertaMaskAddedAttentionHeadRegressor(nn.Module):\n",
    "    def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None, roberta_hidden_dropout_prob=0.1,\n",
    "                 roberta_attention_probs_dropout_prob=0.1, **kwargs):\n",
    "        super(RobertaMaskAddedAttentionHeadRegressor, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(model_path,\n",
    "                                                 hidden_dropout_prob=roberta_hidden_dropout_prob,\n",
    "                                                 attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n",
    "        self.head = MaskAddedAttentionHead(input_dim=self.roberta.config.hidden_size, head_hidden_dim=head_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        roberta_outputs = self.roberta(**inputs)\n",
    "        last_hidden_state = roberta_outputs['last_hidden_state']\n",
    "        attentive_vector = self.head(last_hidden_state, torch.unsqueeze(inputs['attention_mask'], dim=2))\n",
    "        attentive_vector = self.dropout(attentive_vector)\n",
    "        logits = self.regressor(attentive_vector)\n",
    "        return logits\n",
    "```\n",
    "---\n",
    "\n",
    "```python\n",
    "    class RobertaLastHiddenStateMeanPooler(nn.Module):\n",
    "        def __init__(self, model_path, dropout_prob=0.1, head_hidden_dim=None,  roberta_hidden_dropout_prob=0.1, roberta_attention_probs_dropout_prob=0.1, **kwargs):\n",
    "            super(RobertaLastHiddenStateMeanPooler, self).__init__()\n",
    "            self.roberta = AutoModel.from_pretrained(model_path,\n",
    "                                                     hidden_dropout_prob=roberta_hidden_dropout_prob,\n",
    "                                                     attention_probs_dropout_prob=roberta_attention_probs_dropout_prob, **kwargs)\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            roberta_outputs = self.roberta(**inputs)\n",
    "            last_hidden_state = roberta_outputs['last_hidden_state']\n",
    "            masked_last_hidden_state = last_hidden_state * torch.unsqueeze(inputs['attention_mask'], dim=2)\n",
    "            num_tokens = torch.unsqueeze(inputs['attention_mask'], dim=2)\n",
    "            num_tokens = torch.clamp(num_tokens, min=1e-9)\n",
    "            mean_embeddings = masked_last_hidden_state.sum(axis=1) / num_tokens.sum(axis=1)\n",
    "            mean_embeddings = self.dropout(mean_embeddings)\n",
    "            logits = self.regressor(mean_embeddings)\n",
    "            return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3c730-0e87-498e-aa7b-604f5f848650",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Explanation of how Attention Head works\n",
    "The transformer model takes in a batch of `input_ids` of shape `(batch_size, num_tokens)` and gives out the `last_hidden_state` of shape `(batch_size, input_dim)`. Now the attention head takes this `last_hidden_shape` matrix applies attention over and returns a single vector which represents the entire text. Here is how it works:  \n",
    "\n",
    "![image info](images/attention_head.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1e567-3737-44f5-a566-b3dff7bbb32f",
   "metadata": {},
   "source": [
    "## Problem with Attention head -> Mask Added Attention Head to rescue\n",
    "If you look at it, you will notice that the PAD tokens have a **non-zero** vector corresponding to it. Therefore when the final vector is calculated depending on the number of pad tokens which depends on the batch the final vector computation will be different. Therefore the trick of adding the attention_mask to attention_score before calculating the final vector mitigates this. \n",
    "`attention_scores = attention_scores + attention_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e643cb-bb01-451a-b189-751576d71728",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = RobertaMaskAddedAttentionHeadRegressor\n",
    "config = {'base_model_path': 'data/robertas/roberta-base',\n",
    "          'head_hidden_dim': 512,\n",
    "          'dropout_prob': 0.0,\n",
    "          'roberta_hidden_dropout_prob': 0.0,\n",
    "          'roberta_attention_probs_dropout_prob': 0.0,\n",
    "          'layer_norm_eps': 1e-7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7261f6d8-c74b-469e-b37b-9a73758d251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cuda()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model_class(model_path=config['base_model_path'],\n",
    "                    head_hidden_dim=config['head_hidden_dim'],\n",
    "                    dropout_prob=config['dropout_prob'],\n",
    "                    roberta_hidden_dropout_prob=config['roberta_hidden_dropout_prob'],\n",
    "                    roberta_attention_probs_dropout_prob=config['roberta_attention_probs_dropout_prob'],\n",
    "                    layer_norm_eps=config['layer_norm_eps'])\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2e385a-a111-49ed-ac91-88b0b2d9c097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1498],\n",
       "        [0.0915],\n",
       "        [0.0846],\n",
       "        [0.0563]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tokenizer(text=next(iter(train_dataloader))['text_excerpt'], truncation=True, padding=True, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78c8f0-58dd-4fa2-971f-ad1fa09e8cef",
   "metadata": {},
   "source": [
    "# Optimizer - Layerwise Learning Rate Decay\n",
    "\n",
    "```python\n",
    "    def get_optimizer_parameters(group_mode, lr, model, **kwargs):\n",
    "        # group_mode == 'be_wd':\n",
    "        multiplicative_factor = kwargs['multiplicative_factor']\n",
    "        weight_decay = kwargs['weight_decay']\n",
    "        max_num_layers = model.roberta.config.num_hidden_layers\n",
    "\n",
    "        # Task Specific Layer group\n",
    "        tsl_param_group = [{'params': [param for name, param in model.named_parameters() if 'roberta' not in name],\n",
    "                            'param_names': [name for name, param in model.named_parameters() if 'roberta' not in name],\n",
    "                            'lr': lr,\n",
    "                            'name': 'tsl'}]\n",
    "\n",
    "        # Roberta Layer group\n",
    "        roberta_layers_param_groups = []\n",
    "        for layer_num in reversed(range(max_num_layers)):\n",
    "            roberta_layer_param_groups = {'params': [param for name, param in model.named_parameters()\n",
    "                                                     if f'roberta.encoder.layer.{layer_num}.' in name],\n",
    "                                          'param_names': [name for name, param in model.named_parameters()\n",
    "                                                          if f'roberta.encoder.layer.{layer_num}.' in name],\n",
    "                                          'lr': lr * (multiplicative_factor ** (max_num_layers - layer_num)),\n",
    "                                          'name': f'layer_{layer_num}'}\n",
    "            roberta_layers_param_groups.append(roberta_layer_param_groups)\n",
    "\n",
    "        # Embeddding group\n",
    "        embedding_lr = lr * (multiplicative_factor ** (max_num_layers + 1))\n",
    "        embedding_param_group = [{'params': [param for name, param in model.named_parameters() if 'embedding' in name],\n",
    "                                  'param_names': [name for name, param in model.named_parameters() if 'embedding' in name],\n",
    "                                  'lr': embedding_lr,\n",
    "                                  'name': 'embedding'}]\n",
    "\n",
    "        param_groups = tsl_param_group + roberta_layers_param_groups + embedding_param_group\n",
    "        optimizer_parameters = list(chain(*[split_into_wd_groups(param_group, weight_decay=weight_decay)\n",
    "                                            for param_group in param_groups]))\n",
    "        return optimizer_parameters\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "    def split_into_wd_groups(param_group, weight_decay):\n",
    "        # Applies weight decay\n",
    "        weight_parameters = {'params': [param_group['params'][index]\n",
    "                                        for index, name in enumerate(param_group['param_names'])\n",
    "                                        if 'weight' in name and 'LayerNorm' not in name],\n",
    "                             'param_names': [param_group['param_names'][index] \n",
    "                                             for index, name in enumerate(param_group['param_names'])\n",
    "                                             if 'weight' in name and 'LayerNorm' not in name],\n",
    "                             'lr': param_group['lr'],\n",
    "                             'weight_decay': weight_decay,\n",
    "                             'name': param_group['name']+'_weight'}\n",
    "        # Does not apply weight decay\n",
    "        bias_ln_parameters = {'params': [param_group['params'][index]\n",
    "                                         for index, name in enumerate(param_group['param_names'])\n",
    "                                         if 'bias' in name or 'LayerNorm' in name],\n",
    "                              'param_names': [param_group['param_names'][index]\n",
    "                                              for index, name in enumerate(param_group['param_names'])\n",
    "                                              if 'bias' in name or 'LayerNorm' in name],\n",
    "                              'lr': param_group['lr'],\n",
    "                              'weight_decay': 0.0,\n",
    "                              'name': param_group['name']+'_bias_ln'}\n",
    "        parameters = [weight_parameters, bias_ln_parameters]\n",
    "        return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4c9b3c4-3e1d-4f8b-b34b-cf0760cddbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({'group_mode': 'be_wd',\n",
    "               'lr': 3e-5,\n",
    "               'multiplicative_factor': 0.95,\n",
    "               'weight_decay': 0.01,\n",
    "               'eps': 1e-7,\n",
    "               'scheduler_type': 'cosine_schedule_with_warmup',\n",
    "               'num_warmup_steps': 0,\n",
    "               'num_epochs': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b1d516f-c4c0-4b32-baf7-b89b763f67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_parameters = get_optimizer_parameters(group_mode=config['group_mode'], lr=config['lr'],\n",
    "                                                model=model, multiplicative_factor=config['multiplicative_factor'], \n",
    "                                                weight_decay=config['weight_decay'])\n",
    "optimizer = AdamW(optimizer_parameters, eps=config['eps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d2dd021-9810-4f92-bdde-1757e5f8c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_scheduler(scheduler_type=config['scheduler_type'], optimizer=optimizer,\n",
    "                          num_warmup_steps=config['num_warmup_steps'],\n",
    "                          num_training_steps=config['num_epochs'] * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa1c3b07-d414-41cc-a1a1-582b87be461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({'should_save_best_valid_loss_model': False,\n",
    "               'should_save_best_valid_score_model': False,\n",
    "               'should_save_final_model': False,\n",
    "               'save_name': 'test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b459247e-1eb0-40e7-b0c4-3b6f4dc1c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_saver = Saver(metric_name='rmse_loss', is_lower_better=True, config=config, save_name=config['save_name'], should_save=config['should_save_best_valid_loss_model'])\n",
    "valid_score_saver = Saver(metric_name='rmse_score', is_lower_better=True, config=config, save_name=config['save_name'], should_save=config['should_save_best_valid_score_model'])\n",
    "final_model_saver = Saver(metric_name='final_model', is_lower_better=True, config=config, save_name=config['save_name'], should_save=config['should_save_final_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0b88b9-d116-4876-882e-fb43adbed54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({'max_length': 256,\n",
    "               'accumulation_steps': 1,\n",
    "               'validate_every_n_iteration': 1,\n",
    "               'validate_after_n_iteration': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d1290-4e8b-4cb8-a811-1e9e4833e0dc",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "![image info](images/comparator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f9a1b6-4e29-417c-a1e9-bc430e39c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_and_evaluate(num_epochs=config['num_epochs'], train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,\n",
    "                       tokenizer=tokenizer, model=model, optimizer=optimizer, scheduler=scheduler,\n",
    "                       forward_pass_fn_train=forward_pass_uno_text_batch, forward_pass_fn_valid=forward_pass_uno_text_batch,\n",
    "                       compute_loss_fn_train=compute_mse_loss, compute_loss_fn_valid=compute_rmse_loss,\n",
    "                       compute_metric_fn=compute_rmse_score, stacker_class=UnoStacker, max_length=config['max_length'],\n",
    "                       accumulation_steps=config['accumulation_steps'], validate_every_n_iteraion=config['validate_every_n_iteration'],\n",
    "                       validate_after_n_iteration=config['validate_after_n_iteration'],\n",
    "                       valid_loss_saver=valid_loss_saver, valid_score_saver=valid_score_saver, final_model_saver=final_model_saver,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a7714-3723-47b8-8b0f-924cc49ae134",
   "metadata": {},
   "source": [
    "# Dashboards to show\n",
    "1. Rerun experiments for multiple random seeds\n",
    "2. Experiments with and without dropouts\n",
    "3. Experiments with and without LLRD\n",
    "4. Experiment with and without pretrained model\n",
    "\n",
    "WandB dashboards are here: https://wandb.ai/vigneshbaskaran/kaggle_commonlit/reports/Southies-Kaggle-Commonlit--Vmlldzo5OTI5MjU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234011f-8743-4fb5-8e98-27f9fb294082",
   "metadata": {},
   "source": [
    "# Snippets from the dashboard\n",
    "See more from the WandB report: https://wandb.ai/vigneshbaskaran/kaggle_commonlit/reports/Southies-Kaggle-Commonlit--Vmlldzo5OTI5MjU\n",
    "![image info](images/random_seed.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_commonlit",
   "language": "python",
   "name": "kaggle_commonlit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
