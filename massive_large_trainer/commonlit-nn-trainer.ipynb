{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The objective of this notebook is to train a five fold model with the best hyper-parameter settings that we are aware of for now. In the next step we shall re-run this notebook for the full dataset and compare the results to see if training on full dataset has resulted in a jump. \n",
    "\n",
    "The current known best settings are:\n",
    "1. optimizer group: `be_wd`\n",
    "2. learning rate: `3e-5`\n",
    "3. multiplicative factor: `0.925`\n",
    "4. weight decay: `0.02`\n",
    "5. scheduler: `cosine_annealing_with_warmup`\n",
    "6. model: `RobertaMaskAddedAttentionHeadRegressor` \n",
    "7. accumulation_steps: 1\n",
    "8. All dropout_probs: 0 \n",
    "9. num_epochs: 3\n",
    "\n",
    "Make sure this notebook can train on our usual cross validatin strategy as well as full data seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T17:16:58.690407Z",
     "iopub.status.busy": "2021-07-28T17:16:58.689996Z",
     "iopub.status.idle": "2021-07-28T17:17:07.333938Z",
     "shell.execute_reply": "2021-07-28T17:17:07.332706Z",
     "shell.execute_reply.started": "2021-07-28T17:16:58.690323Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from transformers import AdamW, AutoTokenizer\n",
    "from commonlit_nn_kit import seed_everything, clear_cuda, forward_pass_uno_text_batch\n",
    "from commonlit_nn_kit import get_scheduler, get_optimizer_parameters, create_uno_text_dataloader, get_linear_schedule_with_warmup\n",
    "from commonlit_nn_kit import compute_mse_loss, compute_rmse_loss, compute_rmse_score, train_and_evaluate, Saver, UnoStacker, RobertaMaskAddedAttentionHeadRegressor, RobertaLastHiddenStateMeanPooler\n",
    "os.environ[\"WANDB_API_KEY\"] = '72dd109a7a2f5f8fb4b0f8f15019d9f7ab550da7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_parameters = []\n",
    "\n",
    "count = 0\n",
    "model_size = 'large'\n",
    "\n",
    "for random_state in [42]:\n",
    "    for base_model_path in [f'../input/robertas/roberta-{model_size}', '../input/commonlitmlm/mlm_competition_data']:\n",
    "        for train_on_full_data in [False, True]:\n",
    "            for model_class in [RobertaLastHiddenStateMeanPooler, RobertaMaskAddedAttentionHeadRegressor]:\n",
    "                if train_on_full_data:\n",
    "                    training_data_file_name = 'stratified_simple'\n",
    "                    experiment_parameters.append({'name': f'exp_{count}',\n",
    "                                                  'random_state': random_state,\n",
    "                                                  'train_on_full_data': train_on_full_data,\n",
    "                                                  'base_model_path': base_model_path,\n",
    "                                                  'model_class': model_class,\n",
    "                                                  'training_data_file_name': training_data_file_name})\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for training_data_file_name in ['kfold_simple', 'stratified_simple']:\n",
    "                        experiment_parameters.append({'name': f'exp_{count}',\n",
    "                                                      'random_state': random_state,\n",
    "                                                      'train_on_full_data': train_on_full_data,\n",
    "                                                      'base_model_path': base_model_path,\n",
    "                                                      'model_class': model_class,\n",
    "                                                      'training_data_file_name': training_data_file_name})\n",
    "                        count += 1\n",
    "                        \n",
    "for random_state in [1000]:\n",
    "    for base_model_path in [f'../input/robertas/roberta-{model_size}', '../input/commonlitmlm/mlm_competition_data']:\n",
    "        for train_on_full_data in [False, True]:\n",
    "            for model_class in [RobertaLastHiddenStateMeanPooler, RobertaMaskAddedAttentionHeadRegressor]:\n",
    "                if train_on_full_data:\n",
    "                    training_data_file_name = 'stratified_simple'\n",
    "                    experiment_parameters.append({'name': f'exp_{count}',\n",
    "                                                  'random_state': random_state,\n",
    "                                                  'train_on_full_data': train_on_full_data,\n",
    "                                                  'base_model_path': base_model_path,\n",
    "                                                  'model_class': model_class,\n",
    "                                                  'training_data_file_name': training_data_file_name})\n",
    "                    count += 1\n",
    "                else:\n",
    "                    for training_data_file_name in ['kfold_simple', 'stratified_simple']:\n",
    "                        experiment_parameters.append({'name': f'exp_{count}',\n",
    "                                                      'random_state': random_state,\n",
    "                                                      'train_on_full_data': train_on_full_data,\n",
    "                                                      'base_model_path': base_model_path,\n",
    "                                                      'model_class': model_class,\n",
    "                                                      'training_data_file_name': training_data_file_name})\n",
    "                        count += 1\n",
    "\n",
    "pd.DataFrame(experiment_parameters).to_csv('experiment_parameters.csv')\n",
    "experiment_parameters = experiment_parameters[11:]\n",
    "experiment_parameters = [experiment_parameter for experiment_parameter in experiment_parameters if experiment_parameter['train_on_full_data']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T13:58:42.868726Z",
     "iopub.status.busy": "2021-07-28T13:58:42.868167Z",
     "iopub.status.idle": "2021-07-28T14:01:40.309371Z",
     "shell.execute_reply": "2021-07-28T14:01:40.306468Z",
     "shell.execute_reply.started": "2021-07-28T13:58:42.868649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 2267\n",
      "Length of valid data: 567\n",
      "Number of batches in the train_dataloader: 284\n",
      "Number of batches in the valid_dataloader: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvigneshbaskaran\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">proud-yogurt-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/vigneshbaskaran/commonlit-massive-training-large\" target=\"_blank\">https://wandb.ai/vigneshbaskaran/commonlit-massive-training-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/vigneshbaskaran/commonlit-massive-training-large/runs/1rzx2n54\" target=\"_blank\">https://wandb.ai/vigneshbaskaran/commonlit-massive-training-large/runs/1rzx2n54</a><br/>\n",
       "                Run data is saved locally in <code>/home/jupyter/imported/wandb/run-20210729_175759-1rzx2n54</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1445<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/jupyter/imported/wandb/run-20210729_175759-1rzx2n54/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/jupyter/imported/wandb/run-20210729_175759-1rzx2n54/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>78</td></tr><tr><td>_timestamp</td><td>1627581557</td></tr><tr><td>_step</td><td>4980</td></tr><tr><td>Epoch_num</td><td>0</td></tr><tr><td>iteration_num</td><td>92</td></tr><tr><td>iteration_train_loss</td><td>0.26655</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Epoch_num</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iteration_num</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iteration_train_loss</td><td>█▄▅▂▆▃▃▃▂▃▃▂▃▁▃▃▃▃▂▂▂▃▁▃▃▂▃▁▂▁▃▂▄▂▂▃▃▂▃▂</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">proud-yogurt-9</strong>: <a href=\"https://wandb.ai/vigneshbaskaran/commonlit-massive-training-large/runs/1rzx2n54\" target=\"_blank\">https://wandb.ai/vigneshbaskaran/commonlit-massive-training-large/runs/1rzx2n54</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-585c832634aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                    \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                                    \u001b[0mvalidate_every_n_iteraion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validate_every_n_iteration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_after_n_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validate_after_n_iteration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                                    valid_loss_saver=valid_loss_saver, valid_score_saver=valid_score_saver, final_model_saver=final_model_saver, device=device)\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mclear_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/imported/commonlit_nn_kit.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(num_epochs, train_dataloader, valid_dataloader, tokenizer, model, optimizer, scheduler, forward_pass_fn_train, forward_pass_fn_valid, compute_loss_fn_train, compute_loss_fn_valid, compute_metric_fn, stacker_class, max_length, accumulation_steps, validate_every_n_iteraion, valid_loss_saver, valid_score_saver, device, **kwargs)\u001b[0m\n\u001b[1;32m   1075\u001b[0m                                                           \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_pass_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_pass_fn_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                                                           \u001b[0mcompute_loss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss_fn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m                                                           accumulation_steps=accumulation_steps, device=device, **kwargs)\n\u001b[0m\u001b[1;32m   1078\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Epoch_num'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iteration_num'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miteration_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iteration_train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miteration_train_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/imported/commonlit_nn_kit.py\u001b[0m in \u001b[0;36mtrain_one_batch\u001b[0;34m(iteration_num, batch, tokenizer, model, optimizer, scheduler, forward_pass_fn, compute_loss_fn, max_length, accumulation_steps, device, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     batch_loss = forward_pass_fn(batch=batch, tokenizer=tokenizer, model=model, \n\u001b[1;32m   1022\u001b[0m                                  \u001b[0mcompute_loss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m                                  device=device, **kwargs)['loss']                                            # Forward pass\n\u001b[0m\u001b[1;32m   1024\u001b[0m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m                                                   \u001b[0;31m# Normalize our loss (if averaged)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                                          \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/imported/commonlit_nn_kit.py\u001b[0m in \u001b[0;36mforward_pass_uno_text_batch\u001b[0;34m(batch, tokenizer, model, compute_loss_fn, max_length, device, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_excerpts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_excerpt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompute_loss_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/imported/commonlit_nn_kit.py\u001b[0m in \u001b[0;36mcompute_mse_loss\u001b[0;34m(outputs, targets, device, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "\n",
    "for experiment_parameter in experiment_parameters:\n",
    "    config = {}\n",
    "    config.update(experiment_parameter)\n",
    "    config['model_class'] = experiment_parameter['model_class'].__name__\n",
    "    model_class = experiment_parameter['model_class']\n",
    "    \n",
    "    for fold in range(num_folds):\n",
    "        config['save_name'] = f\"experiments/{experiment_parameter['name']}/fold_{fold}\"\n",
    "        config['run_name'] = f\"{experiment_parameter['name']}_fold_{fold}\"\n",
    "\n",
    "        config.update({\n",
    "            'train_on_sample': False,\n",
    "            'fold': fold, \n",
    "            'apply_preprocessing': False,\n",
    "            'batch_size': 8,\n",
    "            'num_epochs': 3,\n",
    "            'validate_every_n_iteration': 1000 if config['train_on_full_data'] else 10,\n",
    "            'validate_after_n_iteration': 1000 if config['train_on_full_data'] else 500,\n",
    "            'tokenizer_name': f'../input/robertas/roberta-{model_size}',\n",
    "            'dropout_prob': 0.0,\n",
    "            'roberta_hidden_dropout_prob': 0.0,\n",
    "            'roberta_attention_probs_dropout_prob': 0.0,\n",
    "            'layer_norm_eps': 1e-7,\n",
    "            'head_hidden_dim': 512,\n",
    "            'group_mode': 'be_wd',\n",
    "            'lr': 3e-5,\n",
    "            'multiplicative_factor': 0.925,\n",
    "            'eps': 1e-7,\n",
    "            'weight_decay': 0.02,\n",
    "            'scheduler_type': 'cosine_schedule_with_warmup',\n",
    "            'num_warmup_steps': 0,\n",
    "            'should_save_best_valid_loss_model': False,\n",
    "            'should_save_best_valid_score_model': False if config['train_on_full_data'] else True,\n",
    "            'should_save_final_model': True,\n",
    "            'max_length': 256,\n",
    "            'accumulation_steps': 1})\n",
    "\n",
    "\n",
    "        if config['train_on_sample']:\n",
    "            config.update({'num_epochs': 2,\n",
    "                           'sample_size': 7,\n",
    "                           'batch_size': 4,\n",
    "                           'validate_every_n_iteration': 1,\n",
    "                           'validate_after_n_iteration': -1})\n",
    "\n",
    "        seed_everything(seed=config['random_state']+config['fold'])\n",
    "\n",
    "        data = pd.read_csv(f\"../input/commonlit-splits/commonlittrain_{config['training_data_file_name']}.csv\")\n",
    "\n",
    "        if config['train_on_full_data']:\n",
    "            data = data.sample(frac=1, random_state=config['random_state'] + config['fold'])\n",
    "            train_data, valid_data = data[:-2], data[-2:]\n",
    "        else:\n",
    "            train_data, valid_data = data[data['fold']!=config['fold']], data[data['fold']==config['fold']]\n",
    "\n",
    "        if config['train_on_sample']:\n",
    "            train_data = train_data[:config['sample_size']]\n",
    "            valid_data = valid_data[:config['sample_size']]\n",
    "\n",
    "        print(f'Length of train data: {len(train_data)}')\n",
    "        print(f'Length of valid data: {len(valid_data)}')\n",
    "\n",
    "        train_dataloader = create_uno_text_dataloader(data=train_data, batch_size=config['batch_size'], shuffle=True, sampler=None, apply_preprocessing=config['apply_preprocessing'])\n",
    "        valid_dataloader = create_uno_text_dataloader(data=valid_data, batch_size=config['batch_size'], shuffle=False, sampler=None, apply_preprocessing=config['apply_preprocessing'])\n",
    "\n",
    "        print(f'Number of batches in the train_dataloader: {len(train_dataloader)}')\n",
    "        print(f'Number of batches in the valid_dataloader: {len(valid_dataloader)}')\n",
    "\n",
    "        clear_cuda()\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=config['tokenizer_name'])\n",
    "        model = model_class(model_path=config['base_model_path'],\n",
    "                            head_hidden_dim=config['head_hidden_dim'],\n",
    "                            dropout_prob=config['dropout_prob'],\n",
    "                            roberta_hidden_dropout_prob=config['roberta_hidden_dropout_prob'],\n",
    "                            roberta_attention_probs_dropout_prob=config['roberta_attention_probs_dropout_prob'],\n",
    "                            layer_norm_eps=config['layer_norm_eps'])\n",
    "        _ = model.to(device)\n",
    "\n",
    "        optimizer_parameters = get_optimizer_parameters(group_mode=config['group_mode'], lr=config['lr'],\n",
    "                                                        model=model, multiplicative_factor=config['multiplicative_factor'], \n",
    "                                                        weight_decay=config['weight_decay'])\n",
    "        optimizer = AdamW(optimizer_parameters, eps=config['eps'])\n",
    "\n",
    "        scheduler = get_scheduler(scheduler_type=config['scheduler_type'], optimizer=optimizer,\n",
    "                                  num_warmup_steps=config['num_warmup_steps'],\n",
    "                                  num_training_steps=config['num_epochs'] * len(train_dataloader))\n",
    "\n",
    "        valid_loss_saver = Saver(metric_name='rmse_loss', is_lower_better=True, config=config, save_name=config['save_name'], should_save=config['should_save_best_valid_loss_model'])\n",
    "        valid_score_saver = Saver(metric_name='rmse_score', is_lower_better=True, config=config, save_name=config['save_name'], should_save=config['should_save_best_valid_score_model'])\n",
    "        final_model_saver = Saver(metric_name='final_model', is_lower_better=True, config=config, save_name=config['save_name'], should_save=config['should_save_final_model'])\n",
    "\n",
    "        run  = wandb.init(reinit=True,\n",
    "                    project=f\"commonlit-massive-training-{model_size}\",\n",
    "                    config=config,\n",
    "                )\n",
    "        wandb.run.name = config['run_name']\n",
    "\n",
    "        with run:\n",
    "            _ = train_and_evaluate(num_epochs=config['num_epochs'], train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, tokenizer=tokenizer,\n",
    "                                   model=model, optimizer=optimizer, scheduler=scheduler,\n",
    "                                   forward_pass_fn_train=forward_pass_uno_text_batch, forward_pass_fn_valid=forward_pass_uno_text_batch,\n",
    "                                   compute_loss_fn_train=compute_mse_loss, compute_loss_fn_valid=compute_rmse_loss,\n",
    "                                   compute_metric_fn=compute_rmse_score, stacker_class=UnoStacker,\n",
    "                                   max_length=config['max_length'], accumulation_steps=config['accumulation_steps'],\n",
    "                                   validate_every_n_iteraion=config['validate_every_n_iteration'], validate_after_n_iteration=config['validate_after_n_iteration'],\n",
    "                                   valid_loss_saver=valid_loss_saver, valid_score_saver=valid_score_saver, final_model_saver=final_model_saver, device=device)\n",
    "            clear_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
